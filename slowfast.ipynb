{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liu-feng116/EvaluateModels/blob/main/slowfast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6cE32bSjBeU"
      },
      "source": [
        "1.import模块"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAMWjnoZ_f8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f1dcbe-ecf9-438f-9654-ad24fa3c5a37"
      },
      "source": [
        "try:\n",
        "    import torch\n",
        "except ModuleNotFoundError:\n",
        "    !pip install torch torchvision\n",
        "    import os\n",
        "    import sys\n",
        "    import torch\n",
        "    \n",
        "if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\n",
        "    !pip install pytorchvideo\n",
        "else:\n",
        "    need_pytorchvideo=False\n",
        "    try:\n",
        "        # Running notebook locally\n",
        "        import pytorchvideo\n",
        "    except ModuleNotFoundError:\n",
        "        need_pytorchvideo=True\n",
        "    if need_pytorchvideo:\n",
        "        # Install from GitHub\n",
        "        !pip install \"git+https://github.com/facebookresearch/pytorchvideo.git\"\n",
        "\n",
        "import json \n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ") \n",
        "from typing import Dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/pytorchvideo.git\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo.git to /tmp/pip-req-build-pp06c3na\n",
            "  Running command git clone -q https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-req-build-pp06c3na\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting av\n",
            "  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.2 MB 31 kB/s \n",
            "\u001b[?25hCollecting parameterized\n",
            "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pytorchvideo==0.1.3) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo==0.1.3) (1.19.5)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo==0.1.3) (4.62.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo==0.1.3) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo==0.1.3) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo==0.1.3) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.3-py3-none-any.whl size=187960 sha256=34c2d86375328598d1746d870c15f7562a2b6aa4552019f72de11f7deaf97f2b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1t7pb47/wheels/87/af/3d/0f80973f39ae2239c1ee9496b333ef4e90bf2d80d486b50eca\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=f89a233bcb1d287a8c6f111b5482f9f51a13183abdbe616483efb87f8f8a01b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n",
            "Successfully built pytorchvideo fvcore\n",
            "Installing collected packages: pyyaml, portalocker, yacs, iopath, parameterized, fvcore, av, pytorchvideo\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed av-8.0.3 fvcore-0.1.5.post20211023 iopath-0.1.9 parameterized-0.8.1 portalocker-2.3.2 pytorchvideo-0.1.3 pyyaml-6.0 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.\n",
            "  \"The _functional_video module is deprecated. Please use the functional module instead.\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.\n",
            "  \"The _transforms_video module is deprecated. Please use the transforms module instead.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc6a6wjcjWLK"
      },
      "source": [
        "2.下载测试数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZs6M86NjZRF",
        "outputId": "6ad132aa-1fb5-4676-dfbe-bc1efb574c3c"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n",
        "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# 对数据集标签进行处理\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-07 12:49:20--  https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10326 (10K) [text/plain]\n",
            "Saving to: ‘kinetics_classnames.json’\n",
            "\n",
            "\rkinetics_classnames   0%[                    ]       0  --.-KB/s               \rkinetics_classnames 100%[===================>]  10.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-07 12:49:20 (77.8 MB/s) - ‘kinetics_classnames.json’ saved [10326/10326]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbeSl4x_kA0z"
      },
      "source": [
        "3.利用Torch Hub加载预处理模型，这里选择根据tutorial先测试slowfast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qromrhr5kSHP"
      },
      "source": [
        "def get_model(device, model_name):\n",
        "  # 从torch hub导入model\n",
        "  model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=True)\n",
        "  # 设置eval（测试），设置设备\n",
        "  model = model.to(device)\n",
        "  model = model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4KeNNFUkv0A"
      },
      "source": [
        "4.设置输入变换（input transformation）。备注：tutorial写到不同模型的输入处理不同，此处针对slowfast。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgtRX2tQmJI4"
      },
      "source": [
        "\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "alpha = 4\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naglW4njmn3Z"
      },
      "source": [
        "5.加载example video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRWrTPh2muUl",
        "outputId": "88ce3d12-b24c-4b96-a8c2-f43b372be86d"
      },
      "source": [
        "# 下载example video\n",
        "!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-07 12:49:20--  https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 549197 (536K) [video/mp4]\n",
            "Saving to: ‘archery.mp4’\n",
            "\n",
            "\rarchery.mp4           0%[                    ]       0  --.-KB/s               \rarchery.mp4         100%[===================>] 536.33K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-11-07 12:49:20 (15.5 MB/s) - ‘archery.mp4’ saved [549197/549197]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OedL1H7tm3Kg"
      },
      "source": [
        "# 对视频进行切片处理\n",
        "def clip(video_path, start_sec, clip_duration, device):\n",
        "  # 选择要加载的切片的持续时间。\n",
        "  end_sec = start_sec + clip_duration \n",
        "  # 初始化EncodeVideo类\n",
        "  video = EncodedVideo.from_path(video_path)\n",
        "  # 加载所需的切片数据\n",
        "  video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "  # 应用此前设置的输入变换处理切片数据\n",
        "  video_data = transform(video_data)\n",
        "  # 将切片数据加载到cpu内存\n",
        "  inputs = video_data[\"video\"]\n",
        "  inputs = [i.to(device)[None, ...] for i in inputs]\n",
        "  return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmJQpN1vpyaG"
      },
      "source": [
        "6.进行prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMqlw_I4p84a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e505c35-49dc-4e2b-dab5-96c6ac1c63bc"
      },
      "source": [
        "import time\n",
        "# 向model导入切片数据\n",
        "inputs = clip(\"archery.mp4\", 0, clip_duration, \"cpu\")\n",
        "model_list = [\"slowfast_r50\", \"slowfast_r101\"]\n",
        "time_list = []\n",
        "mean_time = 0.0\n",
        "for model_name in model_list:\n",
        "  print(\"-----------------------\")\n",
        "  print(\"The model of %s is running:\" % model_name)\n",
        "  # 开始预测\n",
        "  for i in range(10):\n",
        "    inputs = clip(\"archery.mp4\", 0, clip_duration, \"cpu\")\n",
        "    model = get_model(\"cpu\", model_name)\n",
        "\n",
        "    # 开始计时\n",
        "    start = time.clock()\n",
        "\n",
        "    preds = model(inputs)\n",
        "\n",
        "    # 结束计时\n",
        "    end = time.clock()\n",
        "\n",
        "    # 累计求和\n",
        "    mean_time = mean_time + (end - start)\n",
        "\n",
        "    # 获得预测类别\n",
        "    post_act = torch.nn.Softmax(dim=1)\n",
        "    preds = post_act(preds)\n",
        "    pred_classes = preds.topk(k=5).indices\n",
        "    print(pred_classes)\n",
        "\n",
        "    # 通过kineticss_id_to_classname获取对应indices的label\n",
        "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n",
        "    print(\"%d round:\" % i )\n",
        "    print(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n",
        "  \n",
        "  # 平均\n",
        "  mean_time = mean_time/10\n",
        "  time_list.append(mean_time)\n",
        "  print('time: %e' % mean_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "The model of slowfast_r50 is running:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "0 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "1 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "2 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "3 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "4 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "5 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "6 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "7 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "8 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240,  92, 273]])\n",
            "9 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n",
            "time: 4.546172e+00\n",
            "-----------------------\n",
            "The model of slowfast_r101 is running:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "0 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "1 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "2 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "3 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "4 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "5 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "6 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "7 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "8 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5, 356, 240, 141, 273]])\n",
            "9 round:\n",
            "Predicted labels: archery, throwing axe, playing paintball, golf chipping, riding or walking with horse\n",
            "time: 7.601929e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g5j6cwO6gn3"
      },
      "source": [
        "7.绘制图像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "dg41xsrI6mPF",
        "outputId": "78fac21d-f60f-4ae1-928b-81a6cc259768"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.bar(model_list, time_list, width=0.2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}